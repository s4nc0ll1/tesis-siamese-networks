# -*- coding: utf-8 -*-
"""Redes_Siamesas_entrenamiento.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1stMSkDarl99aacKQhL7wKETVw5eY4_Xe
"""

#Installations needed
!pip install -U sentence-transformers
!pip install torch
!pip install datasets

#Libraries
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
from sentence_transformers import SentenceTransformer, InputExample, losses, models, util
from torch.utils.data import DataLoader
from google.colab import drive
from datasets import Dataset

#Dataframe
# Mount Google Drive
drive.mount("/content/drive")

# Load data
columns =  ['CODIGO BMC','CODIGO CLIENTE','DESCRIPCION_CLIENTE','DESCRIPCION_BMC','DESCRIPCION_SUBYACENTE']
db = pd.read_excel("/content/drive/MyDrive/df_limpio_final.xlsx",names = columns)
db.head()

# Training test
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
from torch.optim import AdamW
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
import time
import random

# Load the pre-trained model
model = SentenceTransformer('hiiamsid/sentence_similarity_spanish_es')

# Prepare your training data
train_examples = []
for _, row in db.iterrows():
    train_examples.append(InputExample(texts=[row['DESCRIPCION_CLIENTE'], row['DESCRIPCION_BMC']], label=1.0))
    random_bmc = db['DESCRIPCION_BMC'].sample().values[0]
    train_examples.append(InputExample(texts=[row['DESCRIPCION_CLIENTE'], random_bmc], label=0.0))

# Shuffle and split the data into train and dev sets
random.shuffle(train_examples)
dev_size = max(2, int(0.1 * len(train_examples)))  # Ensure at least 2 examples for dev set
dev_examples = train_examples[:dev_size]
train_examples = train_examples[dev_size:]

# Create data loader
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)

# Change loss func. as in the original training
train_loss = losses.MultipleNegativesRankingLoss(model)

# Development set for evaluation
evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_examples, name='dev_evaluator')

# Training parameters
num_epochs = 4
evaluation_steps = 1000
warmup_steps = 144

# Fine-tune the model
start_time = time.time()
model.fit(train_objectives=[(train_dataloader, train_loss)],
          evaluator=evaluator,
          epochs=num_epochs,
          evaluation_steps=evaluation_steps,
          warmup_steps=warmup_steps,
          optimizer_class=AdamW,
          optimizer_params={'lr': 2e-5, 'weight_decay': 0.01},
          use_amp=True,  # Use automatic mixed precision
          show_progress_bar=True)
end_time = time.time()

# Calculate and print training time
training_time = end_time - start_time
print(f"Training time: {training_time/60:.2f} minutes")

# Save the fine-tuned model
model.save('/content/drive/MyDrive/fine_tuned_sbert_spanish_model_2')

# Training with MultipleNegativesRankingLoss
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
import random

# Load the pre-trained model
model = SentenceTransformer('hiiamsid/sentence_similarity_spanish_es')

# Prepare your training data
train_examples = []
for _, row in db.iterrows():
    # Positive example (correct pair)
    train_examples.append(InputExample(texts=[row['DESCRIPCION_CLIENTE'], row['DESCRIPCION_BMC']]))

    # Multiple negative examples
    for _ in range(3):  # Add 3 negative examples for each positive
        random_bmc = db['DESCRIPCION_BMC'].sample().values[0]
        if random_bmc != row['DESCRIPCION_BMC']:
            train_examples.append(InputExample(texts=[row['DESCRIPCION_CLIENTE'], random_bmc]))

# Shuffle the examples
random.shuffle(train_examples)

# Create data loader
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)  # Increased batch size

# Use MultipleNegativesRankingLoss
train_loss = losses.MultipleNegativesRankingLoss(model)

# Training parameters
num_epochs = 4
warmup_steps = 100

# Fine-tune the model
model.fit(train_objectives=[(train_dataloader, train_loss)],
          epochs=num_epochs,
          warmup_steps=warmup_steps,
          show_progress_bar=True)

# Save the fine-tuned model
model.save('/content/drive/MyDrive/fine_tuned_sbert_spanish_model_3')

#Training with more Negative examples
# Fine-tuning SBERT model
model_name = 'hiiamsid/sentence_similarity_spanish_es'
word_embedding_model = models.Transformer(model_name)
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

# Create training data
train_examples = []
for _, row in db.iterrows():
    # Positive example (correct pair)
    train_examples.append(InputExample(texts=[row['DESCRIPCION_CLIENTE'], row['DESCRIPCION_BMC']], label=1.0))
     # Multiple negative examples
    for _ in range(3):  # Add 3 negative examples for each positive
        random_bmc = db['DESCRIPCION_BMC'].sample().values[0]
        if random_bmc != row['DESCRIPCION_BMC']:
            train_examples.append(InputExample(texts=[row['DESCRIPCION_CLIENTE'], random_bmc],label=0.0))

# DataLoader to batch our data
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
train_loss = losses.CosineSimilarityLoss(model)

# Fine-tune the model with more epochs
num_epochs = 3
start_time = time.time()
model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=num_epochs, warmup_steps=100)
end_time = time.time()

# Calculate training time
training_time = end_time - start_time
print(f"Training time: {training_time/60:.2f} minutes")

# Save the model
model.save('/content/drive/MyDrive/fine_tuned_sbert_spanish_model_4')